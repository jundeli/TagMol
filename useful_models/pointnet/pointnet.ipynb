{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ...\n",
      "Namespace(batch_size=16, decay_rate=0.0001, epoch=251, gpu='-1', learning_rate=0.001, log_dir=None, lr_decay=0.5, model='pointnet_part_seg', normal=False, npoint=2048, optimizer='Adam', step_size=20)\n",
      "The number of training data is: 13998\n",
      "The number of test data is: 2874\n",
      "No existing model, starting training from scratch...\n",
      "Epoch 1 (1/251):\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/874 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Benny\n",
    "Date: Nov 2019\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "import importlib\n",
    "import shutil\n",
    "import provider\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from data_utils.ShapeNetDataLoader import PartNormalDataset\n",
    "\n",
    "BASE_DIR = '/Users/jundeli/Projects/TagMol/useful_models/pointnet'\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "\n",
    "seg_classes = {'Earphone': [16, 17, 18], 'Motorbike': [30, 31, 32, 33, 34, 35], 'Rocket': [41, 42, 43],\n",
    "               'Car': [8, 9, 10, 11], 'Laptop': [28, 29], 'Cap': [6, 7], 'Skateboard': [44, 45, 46], 'Mug': [36, 37],\n",
    "               'Guitar': [19, 20, 21], 'Bag': [4, 5], 'Lamp': [24, 25, 26, 27], 'Table': [47, 48, 49],\n",
    "               'Airplane': [0, 1, 2, 3], 'Pistol': [38, 39, 40], 'Chair': [12, 13, 14, 15], 'Knife': [22, 23]}\n",
    "seg_label_to_cat = {}  # {0:Airplane, 1:Airplane, ...49:Table}\n",
    "for cat in seg_classes.keys():\n",
    "    for label in seg_classes[cat]:\n",
    "        seg_label_to_cat[label] = cat\n",
    "\n",
    "\n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    new_y = torch.eye(num_classes)[y.cpu().data.numpy(),]\n",
    "    if (y.is_cuda):\n",
    "        return new_y.cuda()\n",
    "    return new_y\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser('Model')\n",
    "    parser.add_argument('--model', type=str, default='pointnet_part_seg', help='model name')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='batch Size during training')\n",
    "    parser.add_argument('--epoch', default=251, type=int, help='epoch to run')\n",
    "    parser.add_argument('--learning_rate', default=0.001, type=float, help='initial learning rate')\n",
    "    parser.add_argument('--gpu', type=str, default='-1', help='specify GPU devices')\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help='Adam or SGD')\n",
    "    parser.add_argument('--log_dir', type=str, default=None, help='log path')\n",
    "    parser.add_argument('--decay_rate', type=float, default=1e-4, help='weight decay')\n",
    "    parser.add_argument('--npoint', type=int, default=2048, help='point Number')\n",
    "    parser.add_argument('--normal', action='store_true', default=False, help='use normals')\n",
    "    parser.add_argument('--step_size', type=int, default=20, help='decay step for lr decay')\n",
    "    parser.add_argument('--lr_decay', type=float, default=0.5, help='decay rate for lr decay')\n",
    "\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "def log_string(str):\n",
    "    logger.info(str)\n",
    "    print(str)\n",
    "\n",
    "'''HYPER PARAMETER'''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "'''CREATE DIR'''\n",
    "timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "exp_dir = Path('./log/')\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "exp_dir = exp_dir.joinpath('part_seg')\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "if args.log_dir is None:\n",
    "    exp_dir = exp_dir.joinpath(timestr)\n",
    "else:\n",
    "    exp_dir = exp_dir.joinpath(args.log_dir)\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "checkpoints_dir = exp_dir.joinpath('checkpoints/')\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "log_dir = exp_dir.joinpath('logs/')\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "'''LOG'''\n",
    "args = parse_args()\n",
    "logger = logging.getLogger(\"Model\")\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "log_string('PARAMETER ...')\n",
    "log_string(args)\n",
    "\n",
    "root = 'data/shapenetcore'\n",
    "\n",
    "TRAIN_DATASET = PartNormalDataset(root=root, npoints=args.npoint, split='trainval', normal_channel=args.normal)\n",
    "trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "TEST_DATASET = PartNormalDataset(root=root, npoints=args.npoint, split='test', normal_channel=args.normal)\n",
    "testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=args.batch_size, shuffle=False, num_workers=10)\n",
    "log_string(\"The number of training data is: %d\" % len(TRAIN_DATASET))\n",
    "log_string(\"The number of test data is: %d\" % len(TEST_DATASET))\n",
    "\n",
    "num_classes = 16\n",
    "num_part = 50\n",
    "\n",
    "'''MODEL LOADING'''\n",
    "MODEL = importlib.import_module(args.model)\n",
    "shutil.copy('models/%s.py' % args.model, str(exp_dir))\n",
    "shutil.copy('models/pointnet2_utils.py', str(exp_dir))\n",
    "\n",
    "classifier = MODEL.get_model(num_part, normal_channel=args.normal)\n",
    "criterion = MODEL.get_loss()\n",
    "classifier.apply(inplace_relu)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(str(exp_dir) + '/checkpoints/best_model.pth')\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    log_string('Use pretrain model')\n",
    "except:\n",
    "    log_string('No existing model, starting training from scratch...')\n",
    "    start_epoch = 0\n",
    "    classifier = classifier.apply(weights_init)\n",
    "\n",
    "if args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        classifier.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.decay_rate\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "\n",
    "def bn_momentum_adjust(m, momentum):\n",
    "    if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "        m.momentum = momentum\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-5\n",
    "MOMENTUM_ORIGINAL = 0.1\n",
    "MOMENTUM_DECCAY = 0.5\n",
    "MOMENTUM_DECCAY_STEP = args.step_size\n",
    "\n",
    "best_acc = 0\n",
    "global_epoch = 0\n",
    "best_class_avg_iou = 0\n",
    "best_inctance_avg_iou = 0\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "mean_correct = []\n",
    "\n",
    "log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "'''Adjust learning rate and BN momentum'''\n",
    "lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "log_string('Learning rate:%f' % lr)\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "if momentum < 0.01:\n",
    "    momentum = 0.01\n",
    "print('BN momentum updated to: %f' % momentum)\n",
    "classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "classifier = classifier.train()\n",
    "\n",
    "'''learning one epoch'''\n",
    "for i, (points, label, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    points = points.data.numpy()\n",
    "    points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n",
    "    points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n",
    "    points = torch.Tensor(points)\n",
    "    points, label, target = points.float(), label.long(), target.long()\n",
    "    points = points.transpose(2, 1)\n",
    "\n",
    "    print(points.shape)\n",
    "\n",
    "#     seg_pred, trans_feat = classifier(points, to_categorical(label, num_classes))\n",
    "#     seg_pred = seg_pred.contiguous().view(-1, num_part)\n",
    "#     target = target.view(-1, 1)[:, 0]\n",
    "#     pred_choice = seg_pred.data.max(1)[1]\n",
    "\n",
    "#     correct = pred_choice.eq(target.data).cpu().sum()\n",
    "#     mean_correct.append(correct.item() / (args.batch_size * args.npoint))\n",
    "#     loss = criterion(seg_pred, target, trans_feat)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# train_instance_acc = np.mean(mean_correct)\n",
    "# log_string('Train accuracy is: %.5f' % train_instance_acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from pointnet_utils import STN3d, STNkd, feature_transform_reguliarzer\n",
    "\n",
    "\n",
    "class get_model(nn.Module):\n",
    "    def __init__(self, part_num=50, normal_channel=True):\n",
    "        super(get_model, self).__init__()\n",
    "        if normal_channel:\n",
    "            channel = 6\n",
    "        else:\n",
    "            channel = 3\n",
    "        self.part_num = part_num\n",
    "        self.stn = STN3d(channel)\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, 512, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(512, 2048, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(2048)\n",
    "        self.fstn = STNkd(k=128)\n",
    "        self.convs1 = torch.nn.Conv1d(4944, 256, 1)\n",
    "        self.convs2 = torch.nn.Conv1d(256, 256, 1)\n",
    "        self.convs3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.convs4 = torch.nn.Conv1d(128, part_num, 1)\n",
    "        self.bns1 = nn.BatchNorm1d(256)\n",
    "        self.bns2 = nn.BatchNorm1d(256)\n",
    "        self.bns3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, point_cloud, label):\n",
    "        B, D, N = point_cloud.size()\n",
    "        trans = self.stn(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 6, 1], expected input[16, 3, 2048] to have 6 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=1'>2</a>\u001b[0m point_cloud \u001b[39m=\u001b[39m points\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=2'>3</a>\u001b[0m label \u001b[39m=\u001b[39m to_categorical(label, num_classes)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39;49m(point_cloud, label)\n",
      "File \u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb Cell 3\u001b[0m in \u001b[0;36mget_model.forward\u001b[0;34m(self, point_cloud, label)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, point_cloud, label):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=37'>38</a>\u001b[0m     B, D, N \u001b[39m=\u001b[39m point_cloud\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jundeli/Projects/TagMol/useful_models/pointnet/pointnet.ipynb#ch0000001?line=38'>39</a>\u001b[0m     trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstn(point_cloud)\n",
      "File \u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/TagMol/useful_models/pointnet/models/pointnet_utils.py:29\u001b[0m, in \u001b[0;36mSTN3d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     28\u001b[0m     batchsize \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     30\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     31\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.8/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.8/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    304\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 6, 1], expected input[16, 3, 2048] to have 6 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "self = get_model(part_num=50, normal_channel=False)\n",
    "point_cloud = points\n",
    "label = to_categorical(label, num_classes)\n",
    "self(point_cloud, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98ee1570fecdb964de496ae5cd8ca6705e5126ae24bf68c085fe27dd5c9aa80d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
