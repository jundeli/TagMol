{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): GATLayer(\n",
       "      (projection): Linear(in_features=7, out_features=160, bias=True)\n",
       "    )\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): GATLayer(\n",
       "      (projection): Linear(in_features=32, out_features=160, bias=True)\n",
       "    )\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): GATLayer(\n",
       "      (projection): Linear(in_features=32, out_features=320, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (property_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from frechetdist import frdist\n",
    "import csv\n",
    "\n",
    "from dataloader import PDBbindPLDataset, Normalize, RandomRotateJitter, ToTensor\n",
    "from model import PointNetEncoder, Generator, Discriminator, EnergyModel, RewardModel\n",
    "import itertools\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "lr             = 1e-6\n",
    "batch_size     = 16\n",
    "max_epoch      = 2000\n",
    "num_workers    = 2\n",
    "ligand_size    = 14\n",
    "x_dim          = 16\n",
    "z_dim          = 64\n",
    "n_pc_points    = 4096\n",
    "conv_dims      = [64, 256, 1024]\n",
    "node_dim       = 64\n",
    "n_atom_types   = 7\n",
    "n_bond_types   = 5\n",
    "\n",
    "dataset        = 'refined'\n",
    "n_critic       = 5\n",
    "lambda_gp      = 10\n",
    "alpha_l2       = 1e-3\n",
    "beta_le        = 1e-5\n",
    "gamma_lr       = 1e-4\n",
    "save_step      = 100\n",
    "\n",
    "atom_decoder = {0: 0, 1: 6, 2: 7, 3: 8, 4: 9, 5: 16, 6:17}\n",
    "bond_decoder = {0: Chem.rdchem.BondType.ZERO,\n",
    "                1: Chem.rdchem.BondType.SINGLE,\n",
    "                2: Chem.rdchem.BondType.DOUBLE,\n",
    "                3: Chem.rdchem.BondType.TRIPLE,\n",
    "                4: Chem.rdchem.BondType.AROMATIC}\n",
    "\n",
    "name           = f\"tagmol-xdim/{x_dim}\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/{dataset}_saved_models\"\n",
    "device         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda           = True if torch.cuda.is_available() else False\n",
    "Tensor         = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Make dataloaders\n",
    "# train_dataset = PDBbindPLDataset(root_dir=f'data/pdbbind/{dataset}-set',\n",
    "#                                 n_points=n_pc_points,\n",
    "#                                 lig_size=ligand_size,\n",
    "#                                 train=True,\n",
    "#                                 transform=transforms.Compose([\n",
    "#                                     Normalize(),\n",
    "#                                     RandomRotateJitter(sigma=0.15),\n",
    "#                                     ToTensor()\n",
    "#                                 ]))\n",
    "proteins = pickle.load(open(f'data/pdbbind/proteins.p', \"rb\"))\n",
    "atoms = pickle.load(open(f'data/pdbbind/atoms.p', \"rb\"))\n",
    "bonds = pickle.load(open(f'data/pdbbind/bonds.p', \"rb\"))\n",
    "train_dataset = Data.TensorDataset(Tensor(proteins), Tensor(atoms), Tensor(bonds))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, drop_last=False)\n",
    "\n",
    "# test_dataset = PDBbindPLDataset(root_dir=f'data/pdbbind/{dataset}-set',\n",
    "#                                 n_points=n_pc_points,\n",
    "#                                 lig_size=ligand_size,\n",
    "#                                 train=False,\n",
    "#                                 transform=transforms.Compose([\n",
    "#                                     Normalize(),\n",
    "#                                     RandomRotateJitter(sigma=0.15),\n",
    "#                                     ToTensor()\n",
    "#                                 ]))\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                         shuffle=True, drop_last=False)\n",
    "\n",
    "\n",
    "# Initialize models and make optimizers.\n",
    "encoder       = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "generator     = Generator(x_dim, z_dim, conv_dims, ligand_size, n_atom_types, n_bond_types)\n",
    "discriminator = Discriminator(c_in=n_atom_types, c_out=node_dim, c_hidden=32, n_relations=n_bond_types, n_layers=3)\n",
    "energy_model  = EnergyModel(x_dim, c_in=n_atom_types, c_out=node_dim, n_relations=n_bond_types, n_layers=3)\n",
    "reward_model  = RewardModel(c_in=n_atom_types, c_out=node_dim, c_hidden=32, n_relations=n_bond_types, n_layers=3)\n",
    "\n",
    "opt_enc_gen   = torch.optim.Adam(itertools.chain(encoder.parameters(), generator.parameters()), lr, (0.9, 0.999))\n",
    "opt_disc      = torch.optim.Adam(discriminator.parameters(), lr, (0.9, 0.999))\n",
    "opt_ene       = torch.optim.Adam(energy_model.parameters(), lr, (0.9, 0.999))\n",
    "opt_rew       = torch.optim.Adam(reward_model.parameters(), lr, (0.9, 0.999))\n",
    "\n",
    "if cuda:\n",
    "    encoder.cuda()\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    energy_model.cuda()\n",
    "    reward_model.cuda()\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "\n",
    "# Initialize model weights.\n",
    "encoder.apply(weights_init_normal)\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "energy_model.apply(weights_init_normal)\n",
    "reward_model.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = PDBbindPLDataset(root_dir=f'data/pdbbind/{dataset}-set',\n",
    "#                                 n_points=n_pc_points,\n",
    "#                                 lig_size=ligand_size,\n",
    "#                                 train=False,\n",
    "#                                 transform=transforms.Compose([\n",
    "#                                     Normalize(),\n",
    "#                                     RandomRotateJitter(sigma=0),\n",
    "#                                     ToTensor()\n",
    "#                                 ]))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10000,\n",
    "#                         shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import pickle\n",
    "# pickle.dump(r_bonds.cpu().numpy(), open(f'data/pdbbind/bonds.p', \"wb\" ))\n",
    "\n",
    "proteins = pickle.load(open(f'data/pdbbind/proteins.p', \"rb\"))\n",
    "atoms = pickle.load(open(f'data/pdbbind/atoms.p', \"rb\"))\n",
    "bonds = pickle.load(open(f'data/pdbbind/bonds.p', \"rb\"))\n",
    "train_dataset = Data.TensorDataset(Tensor(proteins), Tensor(atoms), Tensor(bonds))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model checkpoints from tagmol-xdim/16/refined_saved_models...\n"
     ]
    }
   ],
   "source": [
    "epoch = 2663\n",
    "for model in [encoder, generator, discriminator]: #, energy_model, reward_model\n",
    "    path = os.path.join(models_dir, f'{ligand_size}-{epoch}-{str(type(model)).split(\".\")[-1][:-2]}.ckpt')\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "print(f'Loaded model checkpoints from {models_dir}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start traning...\n",
      "none is good!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:32:51] non-ring atom 8 marked aromatic\n",
      "[21:32:51] non-ring atom 1 marked aromatic\n",
      "[21:32:51] non-ring atom 0 marked aromatic\n",
      "[21:32:51] non-ring atom 6 marked aromatic\n",
      "[21:32:51] non-ring atom 2 marked aromatic\n",
      "[21:32:51] non-ring atom 1 marked aromatic\n",
      "[21:32:51] non-ring atom 1 marked aromatic\n",
      "[21:32:51] non-ring atom 10 marked aromatic\n",
      "[21:32:51] non-ring atom 0 marked aromatic\n",
      "[21:32:51] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4 5 7 8 9\n",
      "[21:32:51] Explicit valence for atom # 0 F, 15, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 S, 18, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 N, 22, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 N, 15, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 O, 13, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 Cl, 15, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 S, 16, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 O, 19, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 S, 14, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 C, 15, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 F, 10, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 C, 11, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 N, 14, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 O, 20, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 C, 19, is greater than permitted\n",
      "[21:32:51] Explicit valence for atom # 0 O, 19, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "print('Start traning...')\n",
    "\n",
    "losses_D = []\n",
    "losses_G = []\n",
    "losses_E = []\n",
    "losses_R = []\n",
    "cur_time = time.strftime(\"%D %H:%M:%S\", time.localtime())\n",
    "epoch_log = f\"{cur_time}\\tepoch {epoch+1}\\t\"\n",
    "\n",
    "for batch, sample_batched in enumerate(train_loader):\n",
    "    protein = sample_batched[0] #sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched[1:] #sample_batched['ligand']\n",
    "    batch_log = f\"{epoch+1}:{batch}\\n\"\n",
    "\n",
    "    bs = protein.size(0)\n",
    "    \n",
    "    # # -----------------------\n",
    "    # #  Train Discriminator\n",
    "    # # -----------------------\n",
    "    # opt_disc.zero_grad()\n",
    "\n",
    "    # # Encode protein features using encoder.\n",
    "    # x = encoder(protein.transpose(2, 1))\n",
    "    # # Sample noise as generator input.\n",
    "    z = Variable(Tensor(np.random.normal(0, 1, (bs, z_dim))))\n",
    "    # f_atoms, f_bonds = generator(x, z)\n",
    "    # # Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "    # f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "    # f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "\n",
    "    # # Validity for real and fake samples.\n",
    "    # r_validity = discriminator((r_atoms, r_bonds))\n",
    "    # f_validity = discriminator((f_atoms, f_bonds))\n",
    "\n",
    "    # # Calculate adient penalty.\n",
    "    # gradient_penalty = compute_gradient_penalty(discriminator, r_atoms, r_bonds, f_atoms, f_bonds)\n",
    "\n",
    "    # # Adversarial loss plus gradient penalty.\n",
    "    # loss_D = -torch.mean(r_validity) + torch.mean(f_validity) + lambda_gp * gradient_penalty\n",
    "    # losses_D.append(loss_D.item())\n",
    "\n",
    "    # loss_D.backward()\n",
    "    # opt_disc.step()\n",
    "\n",
    "    # batch_log += f\"loss_d: {-torch.mean(r_validity).item():.2f}\\t{torch.mean(f_validity).item():.2f}\\t{lambda_gp * gradient_penalty.item():.2f}\\n\"\n",
    "\n",
    "    # # Train other networks every n_critic steps\n",
    "    # if (batch+1) % n_critic == 0 or True:\n",
    "\n",
    "    #     # -------------------------------\n",
    "    #     #  Train Generator and Encoder\n",
    "    #     # -------------------------------\n",
    "    #     opt_enc_gen.zero_grad()\n",
    "        \n",
    "    #     x = encoder(protein.transpose(2, 1))\n",
    "    #     z = Variable(Tensor(np.random.normal(0, 1, (batch_size, z_dim))))\n",
    "    #     f_atoms, f_bonds = generator(x, z)\n",
    "    #     # Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "    #     f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "    #     f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "\n",
    "    #     # Validity for fake samples.\n",
    "    #     f_validity = discriminator((f_atoms, f_bonds))\n",
    "    #     loss_G_fake = - torch.mean(f_validity)\n",
    "\n",
    "    #     # # Energies for real and fake samples.\n",
    "    #     # r_out = energy_model(x, r_atoms, r_bonds)\n",
    "    #     # f_out = energy_model(x, f_atoms, f_bonds)\n",
    "    #     # loss_E = torch.mean(r_out) - torch.mean(f_out) + alpha_l2 * torch.mean(r_out ** 2 + f_out ** 2)\n",
    "\n",
    "    #     # # Properties for real and fake ligands.\n",
    "    #     # r_pred_properties = reward_model(r_atoms, r_bonds)\n",
    "    #     # f_pred_properties = reward_model(f_atoms, f_bonds)\n",
    "\n",
    "    #     # # Get rdkit evaluated property scores.\n",
    "    #     # r_properties, f_properties = compute_rdkit_property(r_atoms, r_bonds, f_atoms, f_bonds)\n",
    "\n",
    "    #     # loss_R = torch.mean((r_pred_properties - r_properties)**2 + \\\n",
    "    #     #                                 (f_pred_properties - f_properties)**2)\n",
    "\n",
    "    #     loss_G = loss_G_fake #+ beta_le*loss_E + gamma_lr*loss_R\n",
    "    #     losses_G.append(loss_G.item())\n",
    "\n",
    "    #     loss_G.backward()\n",
    "    #     opt_enc_gen.step()\n",
    "\n",
    "    #     batch_log += f\"loss_g: {loss_G_fake.item():.2f}\\n\" #\\t{loss_E.item():.2f}\\t{loss_R.item():.2f}\n",
    "\n",
    "        # # ------------------------\n",
    "        # #  Train Energy Network\n",
    "        # # ------------------------\n",
    "        # opt_ene.zero_grad()\n",
    "\n",
    "        # x = encoder(protein.transpose(2, 1))\n",
    "        # f_atoms, f_bonds = generator(x, z)\n",
    "        # # Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "        # f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "        # f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "\n",
    "        # # Energies for real and fake samples.\n",
    "        # r_out = energy_model(x, r_atoms, r_bonds)\n",
    "        # f_out = energy_model(x, f_atoms, f_bonds)\n",
    "        # loss_E = torch.mean(r_out) - torch.mean(f_out) + alpha_l2 * torch.mean(r_out ** 2 + f_out ** 2)\n",
    "        # losses_E.append(loss_E.item())\n",
    "\n",
    "        # loss_E.backward()\n",
    "        # opt_ene.step()\n",
    "\n",
    "        # batch_log += f\"loss_e: {torch.mean(r_out).item():.2f}\\t{- torch.mean(f_out).item():.2f}\\t{alpha_l2 * torch.mean(r_out ** 2 + f_out ** 2).item():.2f}\\n\"\n",
    "\n",
    "\n",
    "        # # ------------------------\n",
    "        # #  Train Reward Network\n",
    "        # # ------------------------\n",
    "        # opt_rew.zero_grad()\n",
    "\n",
    "        # x = encoder(protein.transpose(2, 1))\n",
    "        # f_atoms, f_bonds = generator(x, z)\n",
    "        # # Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "        # f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "        # f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "\n",
    "        # # Properties for real and fake ligands.\n",
    "        # r_pred_properties = reward_model(r_atoms, r_bonds)\n",
    "        # f_pred_properties = reward_model(f_atoms, f_bonds)\n",
    "\n",
    "        # # Get rdkit evaluated property scores.\n",
    "        # r_properties, f_properties = compute_rdkit_property(r_atoms, r_bonds, f_atoms, f_bonds)\n",
    "\n",
    "        # loss_R = torch.mean((r_pred_properties - r_properties)**2 + \\\n",
    "        #                                 (f_pred_properties - f_properties)**2)\n",
    "        # losses_R.append(loss_R.item())\n",
    "\n",
    "        # loss_R.backward()\n",
    "        # opt_rew.step()\n",
    "\n",
    "        # batch_log += f\"loss_r: {torch.mean((r_pred_properties - r_properties)**2 ).item():.2f}\\t{torch.mean((f_pred_properties - f_properties)**2 ).item():.2f}\\n\"\n",
    "        # batch_log += f\"properties: {torch.mean(r_properties, 0)[0].item():.2f}\\t{torch.mean(r_properties, 0)[1].item():.2f}\\t{torch.mean(r_properties, 0)[2].item():.2f}\\t{torch.mean(f_properties, 0)[0].item():.2f}\\t{torch.mean(f_properties, 0)[1].item():.2f}\\t{torch.mean(f_properties, 0)[2].item():.2f}\\n\"\n",
    "\n",
    "\n",
    "        # print_and_save(batch_log, f\"{log_dir}/batch-log.txt\")\n",
    "    x = encoder(protein.transpose(2, 1))\n",
    "    f_atoms, f_bonds = generator(x, z)\n",
    "    # Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "    f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "    f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "\n",
    "    r_edges, r_nodes = torch.max(r_bonds, -1)[1], torch.max(r_atoms, -1)[1]\n",
    "    f_edges, f_nodes = torch.max(f_bonds, -1)[1], torch.max(f_atoms, -1)[1]\n",
    "\n",
    "    # Round adjacency matrix to be symmetric.\n",
    "    f_edges = torch.round((f_edges + f_edges.permute(0, 2, 1))/2).to(torch.int32)\n",
    "\n",
    "    r_mols = [matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy())\n",
    "                                        for n_, e_ in zip(r_nodes, r_edges)]\n",
    "    f_mols = [matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy())\n",
    "                                        for n_, e_ in zip(f_nodes, f_edges)]\n",
    "    val_f_mols = [m for m in f_mols if m != None]\n",
    "    if len(val_f_mols):\n",
    "        print(\"Good finding!\")\n",
    "    break\n",
    "print(\"none is good!\")\n",
    "epoch_log += f\"loss_d:{np.mean(losses_D):.4f}\\t loss_g:{np.mean(losses_G):.4f}\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.74615264, 0.98833093, 0.61392303],\n",
       "       [0.16961917, 0.76530229, 0.48597553],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.36659596, 0.32621578],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.29171803, 0.67771965, 0.60010855],\n",
       "       [0.        , 0.20000393, 0.33886946],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.35452503, 0.15486696, 0.66227731]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Property scores of QED, logP, and SAS.\n",
    "reward(r_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(f_mols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
