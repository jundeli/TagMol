{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# This is an official PyTorch implementation for Target-specific\n",
    "# Generation of Molecules (TagMol)\n",
    "# Author: Junde Li, The Pennsylvania State University\n",
    "# Date: Aug 1, 2022\n",
    "###################################################################\n",
    "\n",
    "import os, time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from dataloader import *\n",
    "from utils import MolecularMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "\n",
    "lr             = 1e-4\n",
    "beta1          = 0.0\n",
    "beta2          = 0.9\n",
    "batch_size     = 2\n",
    "max_epoch      = 400\n",
    "num_workers    = 2\n",
    "ligand_size    = 32\n",
    "x_dim          = 1024\n",
    "z_dim          = 128\n",
    "save_step      = 100\n",
    "conv_dims      = [1024, 2048, 2048, 1024]      \n",
    "\n",
    "atom_decoder = {0: 0, 1: 6, 2: 7, 3: 8, 4: 9, 5: 16, 6:17}\n",
    "bond_decoder = {0: Chem.rdchem.BondType.ZERO,\n",
    "                1: Chem.rdchem.BondType.SINGLE,\n",
    "                2: Chem.rdchem.BondType.DOUBLE,\n",
    "                3: Chem.rdchem.BondType.TRIPLE,\n",
    "                4: Chem.rdchem.BondType.AROMATIC\n",
    "                }\n",
    "\n",
    "name           = \"model/tagmol\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/saved_models\"\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7fc5edabb620>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PDBbindPLDataset(root_dir='data/pdbbind/tiny-set',\n",
    "                                n_points=5000, \n",
    "                                lig_size=9,\n",
    "                                train=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    Normalize(),\n",
    "                                    RandomRotateJitter(0.15),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    protein = sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched['ligand']\n",
    "    break\n",
    "\n",
    "pid = '5l3a'\n",
    "from rdkit.Chem.rdmolfiles import MolFromMol2File\n",
    "ligand = MolFromMol2File(os.path.join('data/pdbbind/tiny-set', f'{pid}/{pid}_ligand.mol2'))\n",
    "ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define Encoder Network\n",
    "# --------------------------\n",
    "\n",
    "class STN(nn.Module):\n",
    "    \"\"\"Spatial transformer network for alignment\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super(STN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        identity = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            identity = identity.cuda()\n",
    "        x = x + identity\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetEncoder(nn.Module):\n",
    "    \"\"\"PointNet Encoder Network for protein embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, channel=4, feature_transform=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.stn = STN(k=3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, x_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(x_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STN(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, N = x.size()\n",
    "        trans = self.stn(x[:,:3,:]) # channel=3 only\n",
    "        x = x.transpose(2, 1)\n",
    "        if D > 3:\n",
    "            feature = x[:, :, 3:]\n",
    "            x = x[:, :, :3]\n",
    "        x = torch.bmm(x, trans) # shape=(B, N, 3)\n",
    "\n",
    "        if D > 3:\n",
    "            x = torch.cat([x, feature], dim=2)\n",
    "        x = x.transpose(2, 1) # shape=(B, D, N)\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # shape=(B, 64, N)\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2, 1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x))) # shape=(B, 128, N)\n",
    "        x = self.bn3(self.conv3(x)) # shape=(B, x_dim, N)\n",
    "        # Aggregate point features by max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0] # shape=(B, x_dim)\n",
    "        x = x.view(-1, x_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Network for generating probabilistic distribution of ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, conv_dims, ligand_size, dataset):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_atom_types = len(dataset.atom_encoder)\n",
    "        self.n_bond_types = len(dataset.bond_encoder)\n",
    "        self.ligand_size = ligand_size\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([x_dim+z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.BatchNorm1d(c1, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.atom_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, self.ligand_size * self.n_atom_types),\n",
    "                          nn.Dropout(p=0.2)\n",
    "                          )\n",
    "        self.bond_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, self.ligand_size * self.ligand_size * self.n_bond_types),\n",
    "                          nn.Dropout(p=0.2)\n",
    "                          )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate protein embedding and noise.\n",
    "        gen_input = torch.cat((x, z), -1)\n",
    "\n",
    "        # Generate atoms and bonds.\n",
    "        out = self.layers(gen_input)\n",
    "        atoms = self.atom_layer(out).view(-1, self.ligand_size, self.n_atom_types)\n",
    "        atoms = nn.Softmax(dim=-1)(atoms)\n",
    "\n",
    "        bonds = self.bond_layer(out).view(-1, self.ligand_size, self.ligand_size, self.n_bond_types)\n",
    "        bonds = (bonds + bonds.permute(0, 2, 1, 3)) / 2.0\n",
    "        bonds = nn.Softmax(dim=-1)(bonds)\n",
    "\n",
    "        return atoms, bonds\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out*n_relations)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, atoms, bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            atoms - One-hot encoded input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "            bonds - One-hot encoded adjacency matrix including self-connections.\n",
    "                    Shape = (B, ligand_size, ligand_size)\n",
    "        \"\"\"\n",
    "        bs, n_nodes = atoms.size(0), atoms.size(1)\n",
    "        node_feats = self.projection(atoms)\n",
    "        node_feats = node_feats.view(bs, n_nodes, self.n_relations, -1)\n",
    "\n",
    "        # Calculate the attention logits for evey bond in the ligand.\n",
    "        # Create a tensor of [W_r*h_i||W_r*h_j] with i and j being the indices of all bonds\n",
    "        edges = bonds.nonzero(as_tuple=False) # shape=(b, n_nodes, n_nodes, r)\n",
    "        node_feats_flat = node_feats.view(bs * n_nodes, self.n_relations, -1)\n",
    "        edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * n_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # return concatenated node_feats indiced by i and j. shape = (n_nodes*n_nodes, r, 2*c_out)\n",
    "\n",
    "        # Calculate attention logit alpha(i, j) for each relation.\n",
    "        attn_logits = torch.einsum('brc,rc->br', a_input, self.a) # shape=(n_nodes*n_nodes, r)\n",
    "        attn_logits = nn.LeakyReLU(0.2)(attn_logits)\n",
    "\n",
    "        # Create attention matrix according to relation types.\n",
    "        attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15) # shape=(b, n_nodes, n_nodes, r)\n",
    "        attn_matrix[bonds==1] = torch.gather(attn_logits, 1, edges[:, -1].view(-1, 1)).view(-1)\n",
    "\n",
    "        # Calculate softmax across bonds with all types.\n",
    "        attn_matrix = attn_matrix.view(bs, n_nodes, -1)\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2).view(bs, n_nodes, n_nodes, self.n_relations)\n",
    "\n",
    "        # Sum over neighbors with all relations.\n",
    "        node_feats = torch.einsum('bijr, bjrc->bic', attn_probs, node_feats)\n",
    "\n",
    "        return node_feats\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator with GATLayer for evaluating EM distance btw real and fake ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.validity_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out, c_out),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(c_out, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, ligand):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Input features of one-hot encoded atom vector\n",
    "            adj - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        x, adj = ligand\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                x= l(x, adj)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        h = torch.cat((torch.mean(x, 1), torch.max(x, 1)[0]), 1)\n",
    "        out = self.validity_layer(h)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EnergyModel(nn.Module):\n",
    "    \"\"\"Energy-based network for measuring relative binding affinity btw protein and ligand.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_dim - Dimension of extracted protein features\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(EnergyModel, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.energy_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out+x_dim, 256),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Linear(256, 32),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Linear(32, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x, y_atoms, y_bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Protein features extracted from PointNetEncoder\n",
    "            y_atoms - Input features of one-hot encoded atom vector\n",
    "            y_bonds - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                y_atoms= l(y_atoms, y_bonds)\n",
    "            else:\n",
    "                y_atoms = l(y_atoms)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        y_feats = torch.cat((torch.mean(y_atoms, 1), torch.max(y_atoms, 1)[0]), 1)\n",
    "\n",
    "        # Fuse features from protein and ligand.\n",
    "        h = torch.cat((x, y_feats), 1)\n",
    "        out = self.energy_layer(h)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward network for evaluating ligand properties of QED, logP and SA.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(RewardModel, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.property_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out, c_out),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(c_out, 3)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Input features of one-hot encoded atom vector\n",
    "            adj - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                x= l(x, adj)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        h = torch.cat((torch.mean(x, 1), torch.max(x, 1)[0]), 1)\n",
    "        properties = self.property_layer(h)\n",
    "\n",
    "        return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7485/1971604481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointNetEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7485/3551014723.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# channel=3 only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7485/3551014723.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    297\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    298\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 299\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "encoder = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "x = encoder(protein.transpose(2, 1))\n",
    "z = torch.rand(batch_size, 128)\n",
    "\n",
    "generator = Generator(x_dim, z_dim, conv_dims, 9, dataset)\n",
    "f_atoms_p, f_bonds_p = generator(x, z)\n",
    "\n",
    "# Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "f_atoms = F.gumbel_softmax(f_atoms_p, tau=1, hard=True)\n",
    "f_bonds = F.gumbel_softmax(f_bonds_p, tau=1, hard=True)\n",
    "discriminator = Discriminator(c_in=7, c_out=32, c_hidden=64, n_relations=5, n_layers=3)\n",
    "r_validity = discriminator((r_atoms, r_bonds))\n",
    "f_validity = discriminator((f_atoms, f_bonds))\n",
    "\n",
    "energy_model = EnergyModel(x_dim, c_in=7, c_out=128, n_relations=5, n_layers=3)\n",
    "out = energy_model(x, r_atoms, r_bonds)\n",
    "\n",
    "reward_model = RewardModel(c_in=7, c_out=32, c_hidden=64, n_relations=5, n_layers=3)\n",
    "properties = reward_model(r_atoms, r_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond = F.gumbel_softmax(f_bonds_p, tau=1, hard=True)[0]\n",
    "f_bond = (bond+bond.permute(1, 0, 2))/2\n",
    "f_bond[1,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def compute_gradient_penalty(discriminator, r_atoms, r_bonds, f_atoms, f_bonds):\n",
    "    \"\"\"Calculates the gradient penalty (L2_norm(dy/dx) - 1)**2\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha_atoms = Tensor(np.random.random((r_atoms.size(0), 1, 1)))\n",
    "    alpha_bonds = alpha_atoms.unsqueeze(-1)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interp_atoms = (alpha_atoms * r_atoms + (1 - alpha_atoms) * f_atoms).requires_grad_(True)\n",
    "    interp_bonds = (alpha_bonds * r_bonds + (1 - alpha_bonds) * f_bonds).requires_grad_(True)\n",
    "\n",
    "    interp_atoms = F.gumbel_softmax(interp_atoms, tau=1, hard=True)\n",
    "    interp_bonds = F.gumbel_softmax(interp_bonds, tau=1, hard=True)\n",
    "\n",
    "    interp_validity = discriminator((interp_atoms, interp_bonds))\n",
    "    fake = Variable(Tensor(r_atoms.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interp_validity,\n",
    "        inputs=(interp_atoms, interp_bonds),\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "        allow_unused=True # adj gradients not used in GATLayer\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrices2mol(node_labels, edge_labels):\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # Keep only non-zero nodes and edges.\n",
    "    idx = np.nonzero(node_labels)[0]\n",
    "    for node_label in node_labels[idx]:\n",
    "        mol.AddAtom(Chem.Atom(atom_decoder[node_label]))\n",
    "    edge_labels = edge_labels[idx][:, idx]\n",
    "    for start, end in zip(*np.nonzero(edge_labels)):\n",
    "        if start < end:\n",
    "            mol.AddBond(int(start), int(end), bond_decoder[edge_labels[start, end]])\n",
    "\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "    except:\n",
    "        mol = None\n",
    "\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit = MolecularMetrics()\n",
    "\n",
    "def reward(mols):\n",
    "    \"\"\"Calaulate property scores of QED, logP, and SAS.\"\"\"\n",
    "    validity = rdkit.valid_scores(mols)\n",
    "    logp = rdkit.water_octanol_partition_coefficient_scores(mols, norm=True)\n",
    "    sas = rdkit.synthetic_accessibility_score_scores(mols, norm=True)\n",
    "    qed = rdkit.quantitative_estimation_druglikeness_scores(mols, norm=True)\n",
    "\n",
    "    properties = np.stack((logp, sas, qed), 1)\n",
    "    return properties\n",
    "\n",
    "def compute_rdkit_property(r_atoms, r_bonds, f_atoms, f_bonds):\n",
    "    # Retrieve non-one-hot embedding atoms and bonds.\n",
    "    r_edges, r_nodes = torch.max(r_bonds, -1)[1], torch.max(r_atoms, -1)[1]\n",
    "    f_edges, f_nodes = torch.max(f_bonds, -1)[1], torch.max(f_atoms, -1)[1]\n",
    "\n",
    "    # Round adjacency matrix to be symmetric.\n",
    "    f_edges = torch.round((f_edges + f_edges.permute(0, 2, 1))/2).to(torch.int32)\n",
    "\n",
    "    r_mols = [matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy())\n",
    "                                        for n_, e_ in zip(r_nodes, r_edges)]\n",
    "    f_mols = [matrices2mol(n_.data.cpu().numpy(), e_.data.cpu().numpy())\n",
    "                                        for n_, e_ in zip(f_nodes, f_edges)]\n",
    "\n",
    "    r_properties = reward(r_mols)\n",
    "    f_properties = reward(f_mols)\n",
    "\n",
    "    return r_properties, f_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_rdkit_property' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7485/3843878205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_properties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rdkit_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_bonds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_bonds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_rdkit_property' is not defined"
     ]
    }
   ],
   "source": [
    "r_properties, f_properties = compute_rdkit_property(r_atoms, r_bonds, f_atoms, f_bonds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7485/4182032313.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Properties for real and fake ligands.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr_pred_properties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_bonds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mf_pred_properties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_bonds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Properties for real and fake ligands.\n",
    "r_pred_properties = reward_model(r_atoms, r_bonds)\n",
    "f_pred_properties = reward_model(f_atoms, f_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
