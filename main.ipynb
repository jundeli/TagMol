{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# This is an official PyTorch implementation for Target-specific\n",
    "# Generation of Molecules (TagMol)\n",
    "# Author: Junde Li, The Pennsylvania State University\n",
    "# Date: Aug 1, 2022\n",
    "###################################################################\n",
    "\n",
    "import os, time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "\n",
    "lr             = 1e-4\n",
    "beta1          = 0.0\n",
    "beta2          = 0.9\n",
    "batch_size     = 16\n",
    "max_epoch      = 400\n",
    "num_workers    = 2\n",
    "ligand_size    = 32\n",
    "x_dim         = 1024\n",
    "z_dim         = 128\n",
    "save_step      = 100\n",
    "conv_dims      = [1024, 2048, 2048, 1024]\n",
    "\n",
    "name           = \"model/tagmol\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/saved_models\"\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBbindPLDataset(root_dir='data/pdbbind/tiny-set',\n",
    "                                n_points=5000, \n",
    "                                lig_size=9,\n",
    "                                train=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    Normalize(),\n",
    "                                    RandomRotateJitter(),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    protein = sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched['ligand']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define Encoder Network\n",
    "# --------------------------\n",
    "\n",
    "class STN(nn.Module):\n",
    "    \"\"\"Spatial transformer network for alignment\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super(STN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        identity = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            identity = identity.cuda()\n",
    "        x = x + identity\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetEncoder(nn.Module):\n",
    "    \"\"\"PointNet Encoder Network for protein embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, channel=4, feature_transform=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.stn = STN(k=3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, x_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(x_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STN(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, N = x.size()\n",
    "        trans = self.stn(x[:,:3,:]) # channel=3 only\n",
    "        x = x.transpose(2, 1)\n",
    "        if D > 3:\n",
    "            feature = x[:, :, 3:]\n",
    "            x = x[:, :, :3]\n",
    "        x = torch.bmm(x, trans) # shape=(B, N, 3)\n",
    "\n",
    "        if D > 3:\n",
    "            x = torch.cat([x, feature], dim=2)\n",
    "        x = x.transpose(2, 1) # shape=(B, D, N)\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # shape=(B, 64, N)\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2, 1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x))) # shape=(B, 128, N)\n",
    "        x = self.bn3(self.conv3(x)) # shape=(B, x_dim, N)\n",
    "        # Aggregate point features by max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0] # shape=(B, x_dim)\n",
    "        x = x.view(-1, x_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Network for generating probabilistic distribution of ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, conv_dims, dataset):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_atom_types = len(dataset.atom_encoder)\n",
    "        self.n_bond_types = len(dataset.bond_encoder)\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([x_dim+z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.BatchNorm1d(c1, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.atom_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * self.n_atom_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "        self.bond_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * ligand_size * self.n_bond_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate protein embedding and noise.\n",
    "        gen_input = torch.cat((x, z), -1)\n",
    "\n",
    "        # Generate atoms and bonds.\n",
    "        out = self.layers(gen_input)\n",
    "        atoms = self.atom_layer(out).view(-1, ligand_size, self.n_atom_types)\n",
    "        atoms = nn.Softmax(dim=-1)(atoms)\n",
    "\n",
    "        bonds = self.bond_layer(out).view(-1, ligand_size, ligand_size, self.n_bond_types)\n",
    "        bonds = (bonds + bonds.permute(0, 2, 1, 3)) / 2.0\n",
    "        bonds = nn.Softmax(dim=-1)(bonds)\n",
    "\n",
    "        return atoms, bonds\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Graph Attention Network for evaluating EM distance between real and fake ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, c_din, c_out, n_relations):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_relations = n_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "x = encoder(protein.transpose(2, 1))\n",
    "z = torch.rand(2, 128)\n",
    "\n",
    "self = Generator(x_dim, z_dim, conv_dims, dataset)\n",
    "f_atoms, f_bonds = self(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out*n_relations)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, atoms, bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            atoms - One-hot encoded input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "            bonds - One-hot encoded adjacency matrix including self-connections.\n",
    "                    Shape = (B, ligand_size, ligand_size)\n",
    "        \"\"\"\n",
    "        bs, n_nodes = atoms.size(0), atoms.size(1)\n",
    "        node_feats = self.projection(atoms)\n",
    "        node_feats = node_feats.view(bs, n_nodes, self.n_relations, -1)\n",
    "\n",
    "        # Calculate the attention logits for evey bond in the ligand.\n",
    "        # Create a tensor of [W_r*h_i||W_r*h_j] with i and j being the indices of all bonds\n",
    "        edges = bonds.nonzero(as_tuple=False) # shape=(b, n_nodes, n_nodes, r)\n",
    "        node_feats_flat = node_feats.view(bs * n_nodes, self.n_relations, -1)\n",
    "        edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * n_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # return concatenated node_feats indiced by i and j. shape = (n_nodes*n_nodes, r, 2*c_out)\n",
    "\n",
    "        # Calculate attention logit alpha(i, j) for each relation.\n",
    "        attn_logits = torch.einsum('brc,rc->br', a_input, self.a) # shape=(n_nodes*n_nodes, r)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Create attention matrix according to relation types.\n",
    "        attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15)\n",
    "        attn_matrix[bonds==1]\n",
    "\n",
    "\n",
    "        return node_feats_flat, attn_logits, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = GATLayer(7, 4, 5)\n",
    "n_nodes = 9\n",
    "atoms, bonds = r_atoms, r_bonds\n",
    "node_feats_flat, attn_logits, edges = self(r_atoms, r_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15)\n",
    "attn_matrix[bonds==1] = torch.gather(attn_logits, 1, edges[:, -1].view(-1, 1)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
      "torch.Size([9, 5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([81, 5, 4])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "print(edge_indices_row)\n",
    "print(node_feats_flat.shape)\n",
    "torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8730e-01, -9.0000e+15,  5.8730e-01,  5.8730e-01,  5.8730e-01,\n",
       "          5.8730e-01,  5.8730e-01, -1.0398e-01,  5.8730e-01],\n",
       "        [-9.0000e+15,  5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01,\n",
       "          5.8730e-01,  5.8730e-01, -1.0398e-01,  5.8730e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01,  5.8730e-01, -9.0000e+15,  5.8730e-01,\n",
       "          5.8730e-01,  5.8730e-01, -9.0000e+15,  5.8730e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01, -9.0000e+15,  5.8730e-01, -9.0000e+15,\n",
       "          5.8730e-01,  5.8730e-01, -1.0398e-01,  5.8730e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01,  5.8730e-01, -9.0000e+15,  5.8730e-01,\n",
       "         -9.0000e+15,  5.8730e-01, -1.0398e-01,  5.8730e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01, -9.0000e+15,\n",
       "          5.8730e-01, -9.0000e+15, -1.0398e-01,  5.8730e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01,\n",
       "         -9.0000e+15,  5.8730e-01, -9.0000e+15, -9.0000e+15],\n",
       "        [ 3.0912e-01,  3.0912e-01, -9.0000e+15,  3.0912e-01,  3.0912e-01,\n",
       "          3.0912e-01, -9.0000e+15, -1.5961e-01,  3.0912e-01],\n",
       "        [ 5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01,  5.8730e-01,\n",
       "          5.8730e-01, -9.0000e+15, -1.0398e-01,  5.8730e-01]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_matrix[0][...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
