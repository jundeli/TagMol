{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# This is an official PyTorch implementation for Target-specific\n",
    "# Generation of Molecules (TagMol)\n",
    "# Author: Junde Li, The Pennsylvania State University\n",
    "# Date: Aug 1, 2022\n",
    "###################################################################\n",
    "\n",
    "import os, time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "\n",
    "lr             = 1e-4\n",
    "beta1          = 0.0\n",
    "beta2          = 0.9\n",
    "batch_size     = 16\n",
    "max_epoch      = 400\n",
    "num_workers    = 2\n",
    "ligand_size    = 32\n",
    "x_dim         = 1024\n",
    "z_dim         = 128\n",
    "save_step      = 100\n",
    "conv_dims      = [1024, 2048, 2048, 1024]\n",
    "\n",
    "name           = \"model/tagmol\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/saved_models\"\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBbindPLDataset(root_dir='data/pdbbind/tiny-set',\n",
    "                                n_points=5000, \n",
    "                                lig_size=9,\n",
    "                                train=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    Normalize(),\n",
    "                                    RandomRotateJitter(),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    protein = sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched['ligand']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define Encoder Network\n",
    "# --------------------------\n",
    "\n",
    "class STN(nn.Module):\n",
    "    \"\"\"Spatial transformer network for alignment\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super(STN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        identity = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            identity = identity.cuda()\n",
    "        x = x + identity\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetEncoder(nn.Module):\n",
    "    \"\"\"PointNet Encoder Network for protein embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, channel=4, feature_transform=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.stn = STN(k=3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, x_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(x_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STN(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, N = x.size()\n",
    "        trans = self.stn(x[:,:3,:]) # channel=3 only\n",
    "        x = x.transpose(2, 1)\n",
    "        if D > 3:\n",
    "            feature = x[:, :, 3:]\n",
    "            x = x[:, :, :3]\n",
    "        x = torch.bmm(x, trans) # shape=(B, N, 3)\n",
    "\n",
    "        if D > 3:\n",
    "            x = torch.cat([x, feature], dim=2)\n",
    "        x = x.transpose(2, 1) # shape=(B, D, N)\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # shape=(B, 64, N)\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2, 1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x))) # shape=(B, 128, N)\n",
    "        x = self.bn3(self.conv3(x)) # shape=(B, x_dim, N)\n",
    "        # Aggregate point features by max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0] # shape=(B, x_dim)\n",
    "        x = x.view(-1, x_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Network for generating probabilistic distribution of ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, conv_dims, dataset):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_atom_types = len(dataset.atom_encoder)\n",
    "        self.n_bond_types = len(dataset.bond_encoder)\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([x_dim+z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.BatchNorm1d(c1, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.atom_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * self.n_atom_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "        self.bond_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * ligand_size * self.n_bond_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate protein embedding and noise.\n",
    "        gen_input = torch.cat((x, z), -1)\n",
    "\n",
    "        # Generate atoms and bonds.\n",
    "        out = self.layers(gen_input)\n",
    "        atoms = self.atom_layer(out).view(-1, ligand_size, self.n_atom_types)\n",
    "        atoms = nn.Softmax(dim=-1)(atoms)\n",
    "\n",
    "        bonds = self.bond_layer(out).view(-1, ligand_size, ligand_size, self.n_bond_types)\n",
    "        bonds = (bonds + bonds.permute(0, 2, 1, 3)) / 2.0\n",
    "        bonds = nn.Softmax(dim=-1)(bonds)\n",
    "\n",
    "        return atoms, bonds\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out*n_relations)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, atoms, bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            atoms - One-hot encoded input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "            bonds - One-hot encoded adjacency matrix including self-connections.\n",
    "                    Shape = (B, ligand_size, ligand_size)\n",
    "        \"\"\"\n",
    "        bs, n_nodes = atoms.size(0), atoms.size(1)\n",
    "        node_feats = self.projection(atoms)\n",
    "        node_feats = node_feats.view(bs, n_nodes, self.n_relations, -1)\n",
    "\n",
    "        # Calculate the attention logits for evey bond in the ligand.\n",
    "        # Create a tensor of [W_r*h_i||W_r*h_j] with i and j being the indices of all bonds\n",
    "        edges = bonds.nonzero(as_tuple=False) # shape=(b, n_nodes, n_nodes, r)\n",
    "        node_feats_flat = node_feats.view(bs * n_nodes, self.n_relations, -1)\n",
    "        edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * n_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # return concatenated node_feats indiced by i and j. shape = (n_nodes*n_nodes, r, 2*c_out)\n",
    "\n",
    "        # Calculate attention logit alpha(i, j) for each relation.\n",
    "        attn_logits = torch.einsum('brc,rc->br', a_input, self.a) # shape=(n_nodes*n_nodes, r)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Create attention matrix according to relation types.\n",
    "        attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15) # shape=(b, n_nodes, n_nodes, r)\n",
    "        attn_matrix[bonds==1] = torch.gather(attn_logits, 1, edges[:, -1].view(-1, 1)).view(-1)\n",
    "\n",
    "        # Calculate softmax across bonds with all types.\n",
    "        attn_matrix = attn_matrix.view(bs, n_nodes, -1)\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2).view(bs, n_nodes, n_nodes, self.n_relations)\n",
    "\n",
    "        # Sum over neighbors with all relations.\n",
    "        node_feats = torch.einsum('bijr, bjrc->bic', attn_probs, node_feats)\n",
    "\n",
    "        return node_feats\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator with GATLayer for evaluating EM distance btw real and fake ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "        # self.gat_layers = []\n",
    "        # for l in range(n_layers):\n",
    "        #     self.gat_layers.append(GATLayer(c_in, c_out, n_relations, alpha=0.2))\n",
    "\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [GATLayer(in_channels, c_out, n_relations)]\n",
    "        self.gat_layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for l in self.gat_layers:\n",
    "            X = l(x, adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "x = encoder(protein.transpose(2, 1))\n",
    "z = torch.rand(2, 128)\n",
    "\n",
    "self = Generator(x_dim, z_dim, conv_dims, dataset)\n",
    "f_atoms, f_bonds = self(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out*n_relations)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, atoms, bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            atoms - One-hot encoded input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "            bonds - One-hot encoded adjacency matrix including self-connections.\n",
    "                    Shape = (B, ligand_size, ligand_size)\n",
    "        \"\"\"\n",
    "        bs, n_nodes = atoms.size(0), atoms.size(1)\n",
    "        node_feats = self.projection(atoms)\n",
    "        node_feats = node_feats.view(bs, n_nodes, self.n_relations, -1)\n",
    "\n",
    "        # Calculate the attention logits for evey bond in the ligand.\n",
    "        # Create a tensor of [W_r*h_i||W_r*h_j] with i and j being the indices of all bonds\n",
    "        edges = bonds.nonzero(as_tuple=False) # shape=(b, n_nodes, n_nodes, r)\n",
    "        node_feats_flat = node_feats.view(bs * n_nodes, self.n_relations, -1)\n",
    "        edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * n_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # return concatenated node_feats indiced by i and j. shape = (n_nodes*n_nodes, r, 2*c_out)\n",
    "\n",
    "        # Calculate attention logit alpha(i, j) for each relation.\n",
    "        attn_logits = torch.einsum('brc,rc->br', a_input, self.a) # shape=(n_nodes*n_nodes, r)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        # Create attention matrix according to relation types.\n",
    "        attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15) # shape=(b, n_nodes, n_nodes, r)\n",
    "        attn_matrix[bonds==1] = torch.gather(attn_logits, 1, edges[:, -1].view(-1, 1)).view(-1)\n",
    "\n",
    "        # Calculate softmax across bonds with all relations.\n",
    "        attn_matrix = attn_matrix.view(bs, n_nodes, -1)\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2).view(bs, n_nodes, n_nodes, self.n_relations)\n",
    "\n",
    "        # Sum over neighbors with all relations.\n",
    "        node_feats = torch.einsum('bijr, bjrc->bic', attn_probs, node_feats)\n",
    "\n",
    "        return node_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = GATLayer(7, 10, 5)\n",
    "atoms, bonds = r_atoms, r_bonds\n",
    "node_feats = self(r_atoms, r_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0503,  0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [-2.3163, -0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  4.9874, -0.0000, -0.0000],\n",
       "        [ 4.4403, -0.0000,  0.0000, -0.0000]])"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.8)\n",
    "input = torch.randn(6, 4)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
