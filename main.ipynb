{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# This is an official PyTorch implementation for Target-specific\n",
    "# Generation of Molecules (TagMol)\n",
    "# Author: Junde Li, The Pennsylvania State University\n",
    "# Date: Aug 1, 2022\n",
    "###################################################################\n",
    "\n",
    "import os, time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "\n",
    "lr             = 1e-4\n",
    "beta1          = 0.0\n",
    "beta2          = 0.9\n",
    "batch_size     = 16\n",
    "max_epoch      = 400\n",
    "num_workers    = 2\n",
    "ligand_size    = 32\n",
    "x_dim         = 1024\n",
    "z_dim         = 128\n",
    "save_step      = 100\n",
    "conv_dims      = [1024, 2048, 2048, 1024]\n",
    "\n",
    "name           = \"model/tagmol\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/saved_models\"\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBbindPLDataset(root_dir='data/pdbbind/tiny-set',\n",
    "                                n_points=5000, \n",
    "                                lig_size=32,\n",
    "                                train=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    Normalize(),\n",
    "                                    RandomRotateJitter(),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    protein = sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched['ligand']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define Encoder Network\n",
    "# --------------------------\n",
    "\n",
    "class STN(nn.Module):\n",
    "    \"\"\"Spatial transformer network for alignment\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super(STN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        identity = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            identity = identity.cuda()\n",
    "        x = x + identity\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetEncoder(nn.Module):\n",
    "    \"\"\"PointNet Encoder Network for protein embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, channel=4, feature_transform=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.stn = STN(k=3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, x_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(x_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STN(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, N = x.size()\n",
    "        trans = self.stn(x[:,:3,:]) # channel=3 only\n",
    "        x = x.transpose(2, 1)\n",
    "        if D > 3:\n",
    "            feature = x[:, :, 3:]\n",
    "            x = x[:, :, :3]\n",
    "        x = torch.bmm(x, trans) # shape=(B, N, 3)\n",
    "\n",
    "        if D > 3:\n",
    "            x = torch.cat([x, feature], dim=2)\n",
    "        x = x.transpose(2, 1) # shape=(B, D, N)\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # shape=(B, 64, N)\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2, 1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x))) # shape=(B, 128, N)\n",
    "        x = self.bn3(self.conv3(x)) # shape=(B, x_dim, N)\n",
    "        # Aggregate point features by max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0] # shape=(B, x_dim)\n",
    "        x = x.view(-1, x_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Network for generating probabilistic distribution of ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, conv_dims, dataset):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_atom_types = len(dataset.atom_encoder)\n",
    "        self.n_bond_types = len(dataset.bond_encoder)\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([x_dim+z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.BatchNorm1d(c1, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.atom_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * self.n_atom_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "        self.bond_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, ligand_size * ligand_size * self.n_bond_types),\n",
    "                          nn.Dropout(p=0.5)\n",
    "                          )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate protein embedding and noise.\n",
    "        gen_input = torch.cat((x, z), -1)\n",
    "\n",
    "        # Generate atoms and bonds.\n",
    "        out = self.layers(gen_input)\n",
    "        atoms = self.atom_layer(out).view(-1, ligand_size, self.n_atom_types)\n",
    "        atoms = nn.Softmax(dim=-1)(atoms)\n",
    "\n",
    "        bonds = self.bond_layer(out).view(-1, ligand_size, ligand_size, self.n_bond_types)\n",
    "        bonds = (bonds + bonds.permute(0, 2, 1, 3)) / 2.0\n",
    "        bonds = nn.Softmax(dim=-1)(bonds)\n",
    "\n",
    "        return atoms, bonds\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Graph Attention Network for evaluating EM distance between real and fake ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, c_din, c_out, n_relations):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.n_relations = n_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "x = encoder(protein.transpose(2, 1))\n",
    "z = torch.rand(8, 128)\n",
    "\n",
    "self = Generator(x_dim, z_dim, conv_dims, dataset)\n",
    "f_atoms, f_bonds = self(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations, alpha=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "        self.leakyrele = nn.LeakyReLU(alpha)\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        def forward(self, atoms, bonds):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                atoms - Input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "                bonds - One-hot encodded adjacency matrix including self-connections. \n",
    "                        Shape = (B, ligand_size, ligand_size)\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
