{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul1512/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "# This is an official PyTorch implementation for Target-specific\n",
    "# Generation of Molecules (TagMol)\n",
    "# Author: Junde Li, The Pennsylvania State University\n",
    "# Date: Aug 1, 2022\n",
    "###################################################################\n",
    "\n",
    "import os, time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Hyperparameters\n",
    "# --------------------------\n",
    "\n",
    "lr             = 1e-4\n",
    "beta1          = 0.0\n",
    "beta2          = 0.9\n",
    "batch_size     = 16\n",
    "max_epoch      = 400\n",
    "num_workers    = 2\n",
    "ligand_size    = 32\n",
    "x_dim         = 1024\n",
    "z_dim         = 128\n",
    "save_step      = 100\n",
    "conv_dims      = [1024, 2048, 2048, 1024]\n",
    "\n",
    "name           = \"model/tagmol\"\n",
    "log_dir        = f\"{name}\"\n",
    "models_dir     = f\"{name}/saved_models\"\n",
    "device         = torch.device(\"cuda:0\")\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PDBbindPLDataset(root_dir='data/pdbbind/tiny-set',\n",
    "                                n_points=5000, \n",
    "                                lig_size=9,\n",
    "                                train=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    Normalize(),\n",
    "                                    RandomRotateJitter(),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2,\n",
    "                        shuffle=True)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    protein = sample_batched['protein']\n",
    "    r_atoms, r_bonds = sample_batched['ligand']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Define Encoder Network\n",
    "# --------------------------\n",
    "\n",
    "class STN(nn.Module):\n",
    "    \"\"\"Spatial transformer network for alignment\"\"\"\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super(STN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k * k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        identity = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n",
    "            batchsize, 1)\n",
    "        if x.is_cuda:\n",
    "            identity = identity.cuda()\n",
    "        x = x + identity\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PointNetEncoder(nn.Module):\n",
    "    \"\"\"PointNet Encoder Network for protein embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, channel=4, feature_transform=False):\n",
    "        super(PointNetEncoder, self).__init__()\n",
    "        self.stn = STN(k=3)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, x_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(x_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STN(k=64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, D, N = x.size()\n",
    "        trans = self.stn(x[:,:3,:]) # channel=3 only\n",
    "        x = x.transpose(2, 1)\n",
    "        if D > 3:\n",
    "            feature = x[:, :, 3:]\n",
    "            x = x[:, :, :3]\n",
    "        x = torch.bmm(x, trans) # shape=(B, N, 3)\n",
    "\n",
    "        if D > 3:\n",
    "            x = torch.cat([x, feature], dim=2)\n",
    "        x = x.transpose(2, 1) # shape=(B, D, N)\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # shape=(B, 64, N)\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2, 1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x))) # shape=(B, 128, N)\n",
    "        x = self.bn3(self.conv3(x)) # shape=(B, x_dim, N)\n",
    "        # Aggregate point features by max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0] # shape=(B, x_dim)\n",
    "        x = x.view(-1, x_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Network for generating probabilistic distribution of ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, z_dim, conv_dims, ligand_size, dataset):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.n_atom_types = len(dataset.atom_encoder)\n",
    "        self.n_bond_types = len(dataset.bond_encoder)\n",
    "        self.ligand_size = ligand_size\n",
    "\n",
    "        layers = []\n",
    "        for c0, c1 in zip([x_dim+z_dim]+conv_dims[:-1], conv_dims):\n",
    "            layers.append(nn.Linear(c0, c1))\n",
    "            layers.append(nn.BatchNorm1d(c1, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.atom_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, self.ligand_size * self.n_atom_types),\n",
    "                          nn.Dropout(p=0.2)\n",
    "                          )\n",
    "        self.bond_layer = nn.Sequential(\n",
    "                          nn.Linear(conv_dims[-1], 2048),\n",
    "                          nn.LeakyReLU(0.2, inplace=True),\n",
    "                          nn.Linear(2048, self.ligand_size * self.ligand_size * self.n_bond_types),\n",
    "                          nn.Dropout(p=0.2)\n",
    "                          )\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        # Concatenate protein embedding and noise.\n",
    "        gen_input = torch.cat((x, z), -1)\n",
    "\n",
    "        # Generate atoms and bonds.\n",
    "        out = self.layers(gen_input)\n",
    "        atoms = self.atom_layer(out).view(-1, self.ligand_size, self.n_atom_types)\n",
    "        atoms = nn.Softmax(dim=-1)(atoms)\n",
    "\n",
    "        bonds = self.bond_layer(out).view(-1, self.ligand_size, self.ligand_size, self.n_bond_types)\n",
    "        bonds = (bonds + bonds.permute(0, 2, 1, 3)) / 2.0\n",
    "        bonds = nn.Softmax(dim=-1)(bonds)\n",
    "\n",
    "        return atoms, bonds\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    \"\"\"Single-head GAT layer for passing messages with dynamical weights.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, n_relations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimensionality of input features\n",
    "            c_out - Dimensionality of output features\n",
    "            n_realtions - Number of relation types between atoms\n",
    "        \"\"\"\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        # Tranaform node_feats to c_out dimenional messages.\n",
    "        self.projection = nn.Linear(c_in, c_out*n_relations)\n",
    "        self.a = nn.Parameter(torch.Tensor(n_relations, 2*c_out))\n",
    "\n",
    "        # Initialization from the original implementation\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "    def forward(self, atoms, bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            atoms - One-hot encoded input features of atom nodes. Shape = (B, ligand_size, c_in)\n",
    "            bonds - One-hot encoded adjacency matrix including self-connections.\n",
    "                    Shape = (B, ligand_size, ligand_size)\n",
    "        \"\"\"\n",
    "        bs, n_nodes = atoms.size(0), atoms.size(1)\n",
    "        node_feats = self.projection(atoms)\n",
    "        node_feats = node_feats.view(bs, n_nodes, self.n_relations, -1)\n",
    "\n",
    "        # Calculate the attention logits for evey bond in the ligand.\n",
    "        # Create a tensor of [W_r*h_i||W_r*h_j] with i and j being the indices of all bonds\n",
    "        edges = bonds.nonzero(as_tuple=False) # shape=(b, n_nodes, n_nodes, r)\n",
    "        node_feats_flat = node_feats.view(bs * n_nodes, self.n_relations, -1)\n",
    "        edge_indices_row = edges[:,0] * n_nodes + edges[:,1]\n",
    "        edge_indices_col = edges[:,0] * n_nodes + edges[:,2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
    "        ], dim=-1) # return concatenated node_feats indiced by i and j. shape = (n_nodes*n_nodes, r, 2*c_out)\n",
    "\n",
    "        # Calculate attention logit alpha(i, j) for each relation.\n",
    "        attn_logits = torch.einsum('brc,rc->br', a_input, self.a) # shape=(n_nodes*n_nodes, r)\n",
    "        attn_logits = nn.LeakyReLU(0.2)(attn_logits)\n",
    "\n",
    "        # Create attention matrix according to relation types.\n",
    "        attn_matrix = attn_logits.new_zeros(bonds.shape).fill_(-9e15) # shape=(b, n_nodes, n_nodes, r)\n",
    "        attn_matrix[bonds==1] = torch.gather(attn_logits, 1, edges[:, -1].view(-1, 1)).view(-1)\n",
    "\n",
    "        # Calculate softmax across bonds with all types.\n",
    "        attn_matrix = attn_matrix.view(bs, n_nodes, -1)\n",
    "        attn_probs = F.softmax(attn_matrix, dim=2).view(bs, n_nodes, n_nodes, self.n_relations)\n",
    "\n",
    "        # Sum over neighbors with all relations.\n",
    "        node_feats = torch.einsum('bijr, bjrc->bic', attn_probs, node_feats)\n",
    "\n",
    "        return node_feats\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator with GATLayer for evaluating EM distance btw real and fake ligands.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.validity_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out, c_out),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(c_out, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Input features of one-hot encoded atom vector\n",
    "            adj - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                x= l(x, adj)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        h = torch.cat((torch.mean(x, 1), torch.max(x, 1)[0]), 1)\n",
    "        out = self.validity_layer(h)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EnergyModel(nn.Module):\n",
    "    \"\"\"Energy-based network for measuring relative binding affinity btw protein and ligand.\"\"\"\n",
    "\n",
    "    def __init__(self, x_dim, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_dim - Dimension of extracted protein features\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(EnergyModel, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.energy_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out+x_dim, 256),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Linear(256, 32),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Linear(32, 1)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x, y_atoms, y_bonds):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Protein features extracted from PointNetEncoder\n",
    "            y_atoms - Input features of one-hot encoded atom vector\n",
    "            y_bonds - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                y_atoms= l(y_atoms, y_bonds)\n",
    "            else:\n",
    "                y_atoms = l(y_atoms)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        y_feats = torch.cat((torch.mean(y_atoms, 1), torch.max(y_atoms, 1)[0]), 1)\n",
    "\n",
    "        # Fuse features from protein and ligand.\n",
    "        h = torch.cat((x, y_feats), 1)\n",
    "        out = self.energy_layer(h)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward network for evaluating ligand properties of QED, logP and SA.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, c_hidden=None, n_relations=5, n_layers=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            c_in - Dimension of input features\n",
    "            c_out - Dimension of output features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            n_relations - Number of bond relations between atoms\n",
    "            n_layers - Number of GAT graph layers\n",
    "        \"\"\"\n",
    "        super(RewardModel, self).__init__()\n",
    "        c_hidden = c_hidden if c_hidden else c_out\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(n_layers-1):\n",
    "            layers += [\n",
    "                GATLayer(in_channels, out_channels, n_relations),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout(0.2)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "\n",
    "        layers.append(GATLayer(in_channels, c_out, n_relations))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.property_layer = nn.Sequential(\n",
    "                                    nn.Linear(2*c_out, c_out),\n",
    "                                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(c_out, 3)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x - Input features of one-hot encoded atom vector\n",
    "            adj - Ligand structure features of one-hot encoded bond adjacency matrix\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, GATLayer):\n",
    "                x= l(x, adj)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        # Aggregate mean and max features across all nodes.\n",
    "        h = torch.cat((torch.mean(x, 1), torch.max(x, 1)[0]), 1)\n",
    "        properties = self.property_layer(h)\n",
    "\n",
    "        return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PointNetEncoder(x_dim, channel=4, feature_transform=True)\n",
    "x = encoder(protein.transpose(2, 1))\n",
    "z = torch.rand(2, 128)\n",
    "\n",
    "generator = Generator(x_dim, z_dim, conv_dims, 9, dataset)\n",
    "f_atoms, f_bonds = generator(x, z)\n",
    "\n",
    "# Hard categorical sampling fake ligands from probabilistic distribution.\n",
    "f_atoms = F.gumbel_softmax(f_atoms, tau=1, hard=True)\n",
    "f_bonds = F.gumbel_softmax(f_bonds, tau=1, hard=True)\n",
    "discriminator = Discriminator(c_in=7, c_out=32, c_hidden=64, n_relations=5, n_layers=3)\n",
    "validity = discriminator(r_atoms, r_bonds)\n",
    "\n",
    "energy_model = EnergyModel(x_dim, c_in=7, c_out=128, n_relations=5, n_layers=3)\n",
    "out = energy_model(x, r_atoms, r_bonds)\n",
    "\n",
    "reward_model = RewardModel(c_in=7, c_out=32, c_hidden=64, n_relations=5, n_layers=3)\n",
    "properties = reward_model(r_atoms, r_bonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0446,  0.0758,  0.1268],\n",
       "        [-0.0213,  0.0921,  0.1336]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
