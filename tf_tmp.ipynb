{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, pickle\n",
    "# # Get all PDB IDs\n",
    "# files = list(set([a.split('-')[0] for a in os.listdir('../data/dataset')]))\n",
    "# # Shuffle\n",
    "# random.shuffle(files)\n",
    "# # Split 70/30\n",
    "# training = files[:7 * len(files) // 10]\n",
    "# validation = files[7 * len(files) // 10:]\n",
    "# # Save the IDs to be consistent\n",
    "# pickle.dump(training, open('../results/training.pkl', 'wb'))\n",
    "# pickle.dump(validation, open('../results/validation.pkl', 'wb'))\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "from rdkit.Chem.rdmolfiles import MolFromMol2File\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix, RemoveHs\n",
    "\n",
    "# Define the atom types we are interested in for the protein\n",
    "atomDict = {'C': np.int8(1), 'O': np.int8(2), 'N': np.int8(3), 'S': np.int8(4), 'P': np.int8(5), 'H': np.int8(6), 'X': np.int8(7)}\n",
    "rAtomDict = {v: k for (k,v) in atomDict.items()}\n",
    "rAtomDict[np.int8(0)] = None\n",
    "\n",
    "# Define atom types and bond orders we are interested in for the ligand\n",
    "# We encode aromatics as a separate bond type (1.5: np.int8(4))\n",
    "bondType = {0.0: np.int8(0), 1.0: np.int8(1), 2.0: np.int8(2), 3.0: np.int8(3), 1.5: np.int8(4)}\n",
    "bondMap = np.vectorize(lambda x: bondType[x])\n",
    "# The None entry in ligAtom is necessary to ensure correct one-hot encoding\n",
    "ligAtom = {None: 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4, 'S': 5, 'X': 6}\n",
    "ligAtomMap = np.vectorize(lambda x: ligAtom[x.GetSymbol()] if x.GetSymbol() in ligAtom else 6)\n",
    "\n",
    "# Create the images to be used\n",
    "def createProteinImage(PDBID, imageSize=10, resolution=2):\n",
    "    # Load the protein by parsing the .pdb file\n",
    "    with open('data/dataset/{}.rec.pdb'.format(PDBID)) as f:\n",
    "        # The 77th character is the atom type, while the 30th through 53 characters are the 3D coordinates in angstroms\n",
    "        protein = [(i[77], np.array([float(k) for k in (i[30:38],i[38:46], i[46:54])])) \\\n",
    "                        for i in f.read().split('\\n') if i[:4] == 'ATOM']\n",
    "    # Use rdkit to load the ligand as well\n",
    "    ligand = MolFromMol2File('../data/dataset/{}.lig.mol2'.format(PDBID), sanitize=False)\n",
    "    # rdkit is unable to process some percentage of structures\n",
    "    if ligand is None:\n",
    "        return None\n",
    "    # remove explicit hydrogens\n",
    "    ligand = RemoveHs(ligand, sanitize=False)\n",
    "    # Get the centroid of the ligand\n",
    "    centroid = np.mean(ligand.GetConformer().GetPositions(), axis=0)\n",
    "    # Calculate the lower bound on atom coordinates that will end up in the protein image\n",
    "    lower = centroid - np.repeat(imageSize // 2 * resolution, 3)\n",
    "    # Translate the protein so that the lower bound corresponds to grid index [0,0,0]\n",
    "    protein = [(i[0], i[-1]-lower) for i in protein]\n",
    "    # Convert 3D coordinates to grid indices\n",
    "    protein = [(i, (j // resolution).astype(np.int8)) for (i,j) in protein]\n",
    "    # Filter atoms which are within the imageSize x imageSize x imageSize box\n",
    "    protein = [i for i in protein if np.all(i[1] >= 0) and np.all(i[1] < imageSize)]\n",
    "    protImage = np.zeros((imageSize, imageSize, imageSize, len(atomDict)+1), dtype=bool)\n",
    "    protImage[:, :, :, 0] = True\n",
    "\n",
    "    for (i,j) in protein:\n",
    "        protImage[j[0], j[1], j[2], :] = [k == atomDict[i] for k in range(len(atomDict)+1)]\n",
    "        \n",
    "    # Get the ligand adjacency matrix with bond orders\n",
    "    adj = GetAdjacencyMatrix(ligand, useBO=True)\n",
    "    \n",
    "    # Now remove atoms with lowest bond order until a maximum of 36 atoms remain\n",
    "    toRemove = []\n",
    "    while adj.shape[0] > 36:\n",
    "        # Get the bond orders of each atom\n",
    "        sums = np.sum(adj, axis=0)\n",
    "        # Find a minimum bond order atom\n",
    "        i = np.argmin(sums)\n",
    "        # Remove that atom from the adjacency matrix\n",
    "        adj = np.concatenate((adj[:i,:], adj[i+1:,:]), axis=0)\n",
    "        adj = np.concatenate((adj[:,:i], adj[:,i+1:]), axis=1)\n",
    "        # Keep track of which atoms have been removed from the adjacency matrix\n",
    "        toRemove.append(i)\n",
    "    \n",
    "    # Standardize the adjacency matrices to a 36x36 matrix\n",
    "    if adj.shape[0] < 36:\n",
    "        result = np.zeros((36, 36))\n",
    "        result[:len(adj), :len(adj)] = adj\n",
    "        adj = result\n",
    "        \n",
    "    # Convert bond orders to categories\n",
    "    bonds = bondMap(adj)\n",
    "    # One-hot encode adjacency matrix\n",
    "    bonds = np.array([[[j == t for t in range(len(bondType))] for j in i] for i in bonds])\n",
    "    # Process the atom types\n",
    "    atomList = list(ligAtomMap(ligand.GetAtoms()))\n",
    "    # Remove atoms in the same order they were removed from the adjacency matrix\n",
    "    for i in toRemove:\n",
    "        del atomList[i]\n",
    "    # Standardize to 36 atoms\n",
    "    atoms = np.zeros(36)\n",
    "    atoms[:len(atomList)] = atomList\n",
    "    # One-hot encode atom types\n",
    "    atoms = np.array([[i == t for t in range(len(ligAtom))] for i in atoms])\n",
    "    return protImage, (bonds, atoms)\n",
    "\n",
    "# p, (b, a) = createProteinImage('1a4k')\n",
    "# print('Protein pocket input shape: ', p.shape)\n",
    "# print('Number of protein atoms: ', np.sum(p[..., 1:]))\n",
    "# print('Ligand adjacency matrix shape: ', b.shape)\n",
    "# print('Number of bonds: ', np.sum(b[..., 1:]))\n",
    "# print('Atom type vector shape: ', a.shape)\n",
    "# print('Number of ligand atoms: ', np.sum(a[:, 1:]))\n",
    "\n",
    "import pickle\n",
    "(trainingData, medusa, training) = pickle.load(open('data/tutorialData.pkl', 'rb'))\n",
    "\n",
    "from typeguard import typechecked\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, Conv3D, \\\n",
    "    LayerNormalization, Add, Flatten, Concatenate, Reshape, MaxPooling3D, Layer, \\\n",
    "        AveragePooling3D, Conv3DTranspose, Softmax, Embedding, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "initializer = tf.keras.initializers.VarianceScaling(scale=0.1)\n",
    "\n",
    "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"Performs spectral normalization on weights.\n",
    "    This wrapper controls the Lipschitz constant of the layer by\n",
    "    constraining its spectral norm, which can stabilize the training of GANs.\n",
    "    See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957).\n",
    "    ```python\n",
    "    net = SpectralNormalization(\n",
    "        tf.keras.layers.Conv2D(2, 2, activation=\"relu\"),\n",
    "        input_shape=(32, 32, 3))(x)\n",
    "    net = SpectralNormalization(\n",
    "        tf.keras.layers.Conv2D(16, 5, activation=\"relu\"))(net)\n",
    "    net = SpectralNormalization(\n",
    "        tf.keras.layers.Dense(120, activation=\"relu\"))(net)\n",
    "    net = SpectralNormalization(\n",
    "        tf.keras.layers.Dense(n_classes))(net)\n",
    "    ```\n",
    "    Arguments:\n",
    "      layer: A `tf.keras.layers.Layer` instance that\n",
    "        has either `kernel` or `embeddings` attribute.\n",
    "      power_iterations: `int`, the number of iterations during normalization.\n",
    "    Raises:\n",
    "      AssertionError: If not initialized with a `Layer` instance.\n",
    "      ValueError: If initialized with negative `power_iterations`.\n",
    "      AttributeError: If `layer` does not has `kernel` or `embeddings` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(self, layer: tf.keras.layers, power_iterations: int = 1, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        if power_iterations <= 0:\n",
    "            raise ValueError(\n",
    "                \"`power_iterations` should be greater than zero, got \"\n",
    "                \"`power_iterations={}`\".format(power_iterations)\n",
    "            )\n",
    "        self.power_iterations = power_iterations\n",
    "        self._initialized = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "        super().build(input_shape)\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\n",
    "\n",
    "        if hasattr(self.layer, \"kernel\"):\n",
    "            self.w = self.layer.kernel\n",
    "        elif hasattr(self.layer, \"embeddings\"):\n",
    "            self.w = self.layer.embeddings\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"{} object has no attribute 'kernel' nor \"\n",
    "                \"'embeddings'\".format(type(self.layer).__name__)\n",
    "            )\n",
    "\n",
    "        self.w_shape = self.w.shape.as_list()\n",
    "\n",
    "        self.u = self.add_weight(\n",
    "            shape=(1, self.w_shape[-1]),\n",
    "            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=False,\n",
    "            name=\"sn_u\",\n",
    "            dtype=self.w.dtype,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "        if training is None:\n",
    "            training = tf.keras.backend.learning_phase()\n",
    "\n",
    "        if training:\n",
    "            self.normalize_weights()\n",
    "\n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
    "\n",
    "    @tf.function\n",
    "    def normalize_weights(self):\n",
    "        \"\"\"Generate spectral normalized weights.\n",
    "        This method will update the value of `self.w` with the\n",
    "        spectral normalized value, so that the layer is ready for `call()`.\n",
    "        \"\"\"\n",
    "\n",
    "        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
    "        u = self.u\n",
    "\n",
    "        with tf.name_scope(\"spectral_normalize\"):\n",
    "            for _ in range(self.power_iterations):\n",
    "                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n",
    "                u = tf.math.l2_normalize(tf.matmul(v, w))\n",
    "\n",
    "            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n",
    "\n",
    "            self.w.assign(self.w / sigma)\n",
    "            self.u.assign(u)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"power_iterations\": self.power_iterations}\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "# Training/evaluation batch size\n",
    "BATCHSIZE = 16\n",
    "# Number of ligand atoms\n",
    "N = 36\n",
    "\n",
    "def energy():\n",
    "    # Protein encoder\n",
    "    start = Input((8,10,10,10))\n",
    "    pooled = Flatten()(start)\n",
    "    for i in [1024]:\n",
    "        for j in range(10):\n",
    "            intermediate = pooled\n",
    "            dropout = Dropout(0.2)(intermediate)\n",
    "            dense = SpectralNormalization(Dense(i, kernel_initializer=initializer))(dropout)\n",
    "            act = LeakyReLU()(dense)\n",
    "            intermediate = act\n",
    "            if j == 0:\n",
    "                pooled = intermediate\n",
    "            else:\n",
    "                pooled += intermediate\n",
    "\n",
    "    # Ligand encoder\n",
    "    bonds = Input((N, N, 5))\n",
    "    atoms = Input((N, 7))\n",
    "    s = Concatenate()([Flatten()(bonds), Flatten()(atoms)])\n",
    "    for i in [1024]:\n",
    "        for j in range(10):\n",
    "            intermediate = s\n",
    "            dropout = Dropout(0.2)(intermediate)\n",
    "            dense = SpectralNormalization(Dense(i, kernel_initializer=initializer))(dropout)\n",
    "            act = LeakyReLU()(dense)\n",
    "            intermediate = act\n",
    "            # Skip connection\n",
    "            if j == 0:\n",
    "                s = intermediate\n",
    "            else:\n",
    "                s += intermediate\n",
    "\n",
    "    # Affinity predictor\n",
    "    s = Concatenate()([pooled, s])\n",
    "    for i in [1024]:\n",
    "        for j in range(10):\n",
    "            intermediate = s\n",
    "            dropout = Dropout(0.2)(intermediate)\n",
    "            dense = SpectralNormalization(Dense(i, kernel_initializer=initializer))(dropout)\n",
    "            act = LeakyReLU()(dense)\n",
    "            intermediate = act\n",
    "            # Skip connection\n",
    "            if j == 0:\n",
    "                s = intermediate\n",
    "            else:\n",
    "                s += intermediate\n",
    "\n",
    "    # 13 energies, 7 summary statistics\n",
    "    bd = SpectralNormalization(Dense(1, kernel_initializer=initializer))(s)\n",
    "    s = SpectralNormalization(Dense(13*7, kernel_initializer=initializer))(s)\n",
    "\n",
    "    return Model(inputs=[start, bonds, atoms], outputs=[s, bd])\n",
    "\n",
    "class Energy(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Energy, self).__init__()\n",
    "        self.energy = energy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(0.000001)\n",
    "\n",
    "model = Energy()\n",
    "import time, random, sys\n",
    "@tf.function\n",
    "def train_step(receptor, bonds, atoms, trueEnergy, bd, model):\n",
    "    with tf.GradientTape() as t:\n",
    "        # Predict MedusaDock statistics (energy) and pK (bdPred)\n",
    "        energy, bdPred = model.energy([receptor, bonds, atoms])\n",
    "        \n",
    "        # L^2 cost for both\n",
    "        cost = tf.reduce_mean(tf.square(energy - trueEnergy), axis=-1)\n",
    "        costbd = tf.square(bd-bdPred)\n",
    "        \n",
    "        # Evaluate gradient\n",
    "        loss = tf.reduce_mean(tf.expand_dims(cost, 1)+costbd)\n",
    "        grad = t.gradient(loss, model.energy.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad, _ = tf.clip_by_global_norm(grad, 0.5)\n",
    "        model.optimizer.apply_gradients(zip(grad, model.energy.trainable_variables))\n",
    "        \n",
    "        # Return RMSE per batch\n",
    "        return tf.sqrt(tf.reduce_mean(cost[..., 0])), tf.sqrt(tf.reduce_mean(costbd))\n",
    "\n",
    "def print_and_save(s, fname):\n",
    "    print(s)\n",
    "    with open(fname,\"a\") as f:\n",
    "        f.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Time for epoch 1 is 2.1920230388641357 sec\n",
      "epoch 1\tloss:29.7674\t bdloss:2.3680\n",
      "Time for epoch 2 is 2.18123459815979 sec\n",
      "epoch 2\tloss:131.5450\t bdloss:2.1761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_202678/705921448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcostbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreceptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbonds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrueEnergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mcostbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcostbd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/miniconda3/envs/kongsr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "EPOCH = 100\n",
    "with open('data/tutorialData.pkl', 'rb') as f:\n",
    "    (trainingData, medusa, training) = pickle.load(f)\n",
    "#trainingData = {i: (X, Y, Z, bd) for (i, (X, Y, Z, bd)) in trainingData.items()}\n",
    "print(\"Start training...\")\n",
    "start_epoch = 0\n",
    "if start_epoch:\n",
    "    model.load_weights(f'data/finalNoTrans/model-epoch{start_epoch}')\n",
    "    print(f\"Load model weights from epoch {start_epoch}\")\n",
    "for epoch in range(start_epoch, EPOCH):\n",
    "    start = time.time()\n",
    "\n",
    "    # Shuffle data\n",
    "    random.shuffle(training)\n",
    "    batches = len(training) // BATCHSIZE\n",
    "    \n",
    "    # Keep track of costs\n",
    "    c = []\n",
    "    cbd = []\n",
    "    curr_log = f\"epoch {epoch+1}\\t\"\n",
    "    for batch in range(batches):\n",
    "        \n",
    "        sIds = training[BATCHSIZE * batch:min(BATCHSIZE * (batch + 1), len(training))]\n",
    "\n",
    "        receptor, bonds, atoms, bd = zip(*[trainingData[sId] for sId in sIds])\n",
    "\n",
    "        receptor = np.concatenate(receptor).astype(np.float32).reshape(-1, 8, 10, 10, 10)\n",
    "        bonds = np.stack(bonds).astype(np.float32)\n",
    "        atoms = np.stack(atoms).astype(np.float32)\n",
    "\n",
    "        # MedusaDock\n",
    "        trueEnergy = np.concatenate([medusa[sId] for sId in sIds], axis=0).astype(np.float32)\n",
    "        \n",
    "        bd = np.array(bd).astype(np.float32)[:, np.newaxis]\n",
    "\n",
    "        cost, costbd = train_step(receptor, bonds, atoms, trueEnergy, bd, model)\n",
    "        cost = cost.numpy()\n",
    "        costbd = costbd.numpy()\n",
    "        \n",
    "        # Keep track of costs\n",
    "        c.append(cost)\n",
    "        cbd.append(costbd)\n",
    "        \n",
    "        # print(f\"Batch {epoch} \\tloss: {cost:.4f} \\tl bd loss: {costbd:.4f}\")\n",
    "\n",
    "    print('Time for epoch {} is {} sec'.format(epoch+1, time.time()-start))\n",
    "    curr_log += f\"loss:{np.mean(c):.4f}\\t bdloss:{np.mean(cbd):.4f}\"\n",
    "    print_and_save(curr_log, \"tf-pdb-cla.txt\")\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d856909ca9685d9bf9012774e4c92e1590f564586ad7784e5945c8d33388486"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('kongsr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
